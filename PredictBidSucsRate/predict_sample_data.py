# -*- coding: utf-8 -*-
"""
ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ìê°€ ì œê³µí•œ sample_prediction_data.csvë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import sys
import pandas as pd
import numpy as np
import joblib
import random as rnd
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix
from kiwipiepy import Kiwi

# KiwiTokenizer í´ë˜ìŠ¤ (bid.ml.train.pyì—ì„œ ë³µì‚¬)
class KiwiTokenizer():
    def __init__(self, saved_filenm):
        self.rnd_num = rnd.randint(100, 999)
        self.cur_dir = os.getcwd()
        self.data_dir = self.cur_dir+'\\data\\'
        self.save_dir = self.cur_dir+'\\res\\'
        self.save_filename = saved_filenm
        self.kiwi = None
        self.kiwi = self.CreateKiwi(saved_filenm)
    
    def save(self, filename):
        filepath = os.path.join(self.save_dir, filename)
        joblib.dump(self.kiwi._user_values, filepath)
        return self
    
    def load(self, filename):
        filepath = os.path.join(self.save_dir, filename)
        o = joblib.load(filepath)
        return o
        
    def loadDictonary(self, filename):
        self.data = pd.read_csv(self.data_dir+filename)
        print(self.data[:10])
        for i, row in enumerate(self.data.iloc):
            self.kiwi.add_user_word(row["ë‹¨ì–´"], 'NNP')
    
    def CreateKiwi(self, saved_filenm):
        o = None
        if(self.kiwi is None):
            if(saved_filenm is None):
                o = Kiwi(num_workers=8)
            else:
                o = Kiwi(num_workers=8)
                o._user_values = self.load(self.save_dir+saved_filenm)
        return o
            
    def cleared_line(self, line):
        if pd.isna(line) or line is None:
            line = ''
        else:
            line = str(line).lower().replace('(', ' ').replace(')', ' ').replace('n/a', '')
        
        nm_words = []
        tokens = self.kiwi.tokenize(line)
        for token in tokens:
            if token.tag in ['MM', 'NNG', 'NNB', 'NNP', 'SL', 'XPN', 'MAG', 'SN', 'SO', 'W_SERIAL']:
                nm_words.append(token.form)
                
        return ' '.join(nm_words)
    
    def cleared_lines_from(self, orglines):
        lines = []
        for line in orglines:
            lines.append(self.cleared_line(line))
        return lines      
    
    def nn_only(self, orglines):
        lines = []
        for key in orglines:
            if pd.isna(key) or key is None:
                key = ''
            else:
                key = str(key).lower().replace('(', ' ').replace(')', ' ').replace('n/a', '')
            
            nm_words = []
            tokens = self.kiwi.tokenize(key)
            for token in tokens:
                if token.tag in ['MM', 'NNG', 'NNB', 'NNP', 'SL', 'XPN', 'MAG', 'SN', 'SO', 'W_SERIAL']:
                    nm_words.append(token.form)
            lines.append(' '.join(nm_words))
        return lines

# KiwiVectorizer í´ë˜ìŠ¤ (bid.ml.train.pyì—ì„œ ë³µì‚¬)
class KiwiVectorizer():
    def __init__(self):
        self.rnd_num = rnd.randint(100, 999)
        self.cur_dir = os.getcwd()
        self.save_dir = self.cur_dir+'\\res\\'
        
        self.vect = TfidfVectorizer(
            ngram_range=(1, 1), 
            token_pattern='(?u)\\b\\w+\\b',
            max_features=5000,
            min_df=2,
            max_df=0.95,
            sublinear_tf=True
        )
    
    def load(self, filename):
        filepath = os.path.join(self.save_dir, filename)
        voca = joblib.load(filepath)
        self.vect.vocabulary_ = voca["vocabulary"] 
        self.vect.idf_ = voca["idf"]
    
    def save(self, filename):
        filepath = os.path.join(self.save_dir, filename)
        joblib.dump({'vocabulary':self.vect.vocabulary_, 'idf':self.vect.idf_}, filepath)
        print("ë‹¨ì–´ì‚¬ì „ íŒŒì¼("+filename+")ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ")
    
    def fit(self, lines):
        return self.vect.fit(lines)
    
    def transform(self, lines):
        return self.vect.transform(lines)
    
    def scores(self, lines):
        return self.toValues(self.transform(lines))

    def toValues(self, csrmat:csr_matrix):
        lst = []
        safty_sz = len(csrmat.indptr)-1
        for i, n in enumerate(csrmat.indptr):
            sumval = 0
            if i<safty_sz:
                indiceVal = csrmat.indices[csrmat.indptr[i]:csrmat.indptr[i+1]]
                dataVal = csrmat.data[csrmat.indptr[i]:csrmat.indptr[i+1]]
                for j, va1 in enumerate(indiceVal):
                    sumval += va1 * dataVal[j]
                lst.append(sumval)
        return lst

class SampleDataPredictor():
    """
    ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ í´ë˜ìŠ¤
    """
    
    def __init__(self):
        print("="*80)
        print("ğŸ”® ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print("="*80)
        
        self.cur_dir = os.getcwd()
        self.data_dir = self.cur_dir + '\\data\\'
        self.save_dir = self.cur_dir + '\\res\\7.7\\'
        
        self.load_models_and_preprocessors()
        print("âœ… ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        print("="*80)
    
    def load_models_and_preprocessors(self):
        """ì €ì¥ëœ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ë„êµ¬ë“¤ì„ ë¡œë“œ"""
        try:
            # 1. í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ
            print("ğŸ“š í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì¤‘...")
            self.tokenizer = KiwiTokenizer("mlpregr.tokenizer.v0.1.1.npz")
            self.tokenizer.loadDictonary('í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „.NNP.csv')
            print("âœ… í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ")
            
            # 2. TF-IDF ë²¡í„°í™”ê¸° ë¡œë“œ
            print("ğŸ”¤ TF-IDF ë²¡í„°í™”ê¸° ë¡œë“œ ì¤‘...")
            self.vectorizer = KiwiVectorizer()
            self.vectorizer.load("mlpregr.vectorizer.v0.1.1.npz")
            print("âœ… TF-IDF ë²¡í„°í™”ê¸° ë¡œë“œ ì™„ë£Œ")
            
            # 3. ì •ê·œí™” ë„êµ¬ ë¡œë“œ
            print("ğŸ“Š ì •ê·œí™” ë„êµ¬ ë¡œë“œ ì¤‘...")
            self.scaler = joblib.load(self.save_dir + "x_fited_scaler.v2.npz")
            print("âœ… ì •ê·œí™” ë„êµ¬ ë¡œë“œ ì™„ë£Œ")
            
            # 4. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ ë¡œë“œ
            print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
            self.model1 = joblib.load(self.save_dir + "mlpregr.model1.v0.1.1.npz")  # ì—…ì²´íˆ¬ì°°ë¥ 
            self.model2 = joblib.load(self.save_dir + "mlpregr.model2.v0.1.1.npz")  # ì˜ˆê°€íˆ¬ì°°ë¥ 
            self.model3 = joblib.load(self.save_dir + "mlpregr.model3.v0.1.1.npz")  # ì°¸ì—¬ì—…ì²´ìˆ˜
            print("âœ… ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ë¨¼ì € bid.ml.train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.")
            sys.exit(1)
    
    def preprocess_data(self, data_file):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("="*80)
        print(f"ğŸ“ ë°ì´í„° ë¡œë“œ: {data_file}")
        print("="*80)
        
        # CSV íŒŒì¼ ë¡œë“œ
        data = pd.read_csv(self.data_dir + data_file)
        print(f"ë°ì´í„° í¬ê¸°: {data.shape}")
        print(f"ì»¬ëŸ¼: {list(data.columns)}")
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°± ì œê±°)
        data.columns = data.columns.str.strip()
        print(f"ì •ë¦¬ëœ ì»¬ëŸ¼: {list(data.columns)}")
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ ì •ì˜
        required_columns = ['ê¸°ì´ˆê¸ˆì•¡', 'ë‚™ì°°í•˜í•œë¥ ', 'ì°¸ì—¬ì—…ì²´ìˆ˜',
                           'ê°„ì ‘ë¹„', 'ìˆœê³µì‚¬ì›ê°€', 
                           'ë©´í—ˆì œí•œì½”ë“œ', 'ê³µê³ ê¸°ê´€ì½”ë“œ',
                           'ê³µê³ ê¸°ê´€ëª…', 'ê³µê³ ê¸°ê´€ì ìˆ˜',
                           'ê³µì‚¬ì§€ì—­', 'ê³µì‚¬ì§€ì—­ì ìˆ˜',
                           'í‚¤ì›Œë“œ', 'í‚¤ì›Œë“œì ìˆ˜']
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in required_columns if col in data.columns]
        missing_columns = [col for col in required_columns if col not in data.columns]
        
        if missing_columns:
            print(f"âš ï¸  ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤: {missing_columns}")
            print("ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›Œì§‘ë‹ˆë‹¤.")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        dataset_x = pd.DataFrame(data, columns=available_columns)
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        for col in missing_columns:
            if col in ['ë©´í—ˆì œí•œì½”ë“œ', 'ê³µê³ ê¸°ê´€ì½”ë“œ']:
                dataset_x[col] = 0
            elif col in ['ê³µê³ ê¸°ê´€ì ìˆ˜', 'ê³µì‚¬ì§€ì—­ì ìˆ˜', 'í‚¤ì›Œë“œì ìˆ˜']:
                dataset_x[col] = 0.0
            elif col in ['ê³µê³ ê¸°ê´€ëª…', 'ê³µì‚¬ì§€ì—­', 'í‚¤ì›Œë“œ']:
                dataset_x[col] = ''
            elif col == 'ì°¸ì—¬ì—…ì²´ìˆ˜':
                dataset_x[col] = 5  # ê¸°ë³¸ê°’ìœ¼ë¡œ 5 ì„¤ì •
            else:
                dataset_x[col] = 0
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        dataset_x = dataset_x[required_columns]
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        print("ğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        text_columns = ['í‚¤ì›Œë“œ', 'ê³µê³ ê¸°ê´€ëª…', 'ê³µì‚¬ì§€ì—­']
        for col in text_columns:
            if col in dataset_x.columns:
                dataset_x[col] = dataset_x[col].fillna('').astype(str)
                dataset_x[col] = dataset_x[col].replace(['nan', 'NaN', 'None', 'null'], '')
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ë“¤ì„ ìˆ«ìë¡œ ë³€í™˜
        print("ğŸ”¢ ìˆ«ì ë°ì´í„° ë³€í™˜ ì¤‘...")
        if 'ë©´í—ˆì œí•œì½”ë“œ' in dataset_x.columns:
            dataset_x['ë©´í—ˆì œí•œì½”ë“œ'] = dataset_x['ë©´í—ˆì œí•œì½”ë“œ'].astype(str).apply(
                lambda x: hash(x) % 1000000 if pd.notna(x) and x != 'nan' else 0)
        
        if 'ê³µê³ ê¸°ê´€ì½”ë“œ' in dataset_x.columns:
            dataset_x['ê³µê³ ê¸°ê´€ì½”ë“œ'] = dataset_x['ê³µê³ ê¸°ê´€ì½”ë“œ'].astype(str).apply(
                lambda x: hash(x) % 1000000 if pd.notna(x) and x != 'nan' else 0)
        
        # í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ TF-IDF ì ìˆ˜ë¡œ ë³€í™˜
        print("ğŸ”¤ í…ìŠ¤íŠ¸ë¥¼ TF-IDF ì ìˆ˜ë¡œ ë³€í™˜ ì¤‘...")
        lines = self.tokenizer.nn_only(dataset_x["í‚¤ì›Œë“œ"].tolist())
        lines2 = self.tokenizer.nn_only(dataset_x["ê³µê³ ê¸°ê´€ëª…"].tolist())
        lines3 = self.tokenizer.nn_only(dataset_x["ê³µì‚¬ì§€ì—­"].tolist())
        
        # TF-IDF ì ìˆ˜ ê³„ì‚°
        pts = self.vectorizer.scores(lines)
        pts2 = self.vectorizer.scores(lines2)
        pts3 = self.vectorizer.scores(lines3)
        
        # ì ìˆ˜ë¥¼ ë°ì´í„°ì— ì¶”ê°€
        dataset_x["í‚¤ì›Œë“œì ìˆ˜"] = pts
        dataset_x["ê³µê³ ê¸°ê´€ì ìˆ˜"] = pts2
        dataset_x["ê³µì‚¬ì§€ì—­ì ìˆ˜"] = pts3
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {dataset_x.shape[1]}ê°œ íŠ¹ì„±")
        return dataset_x
    
    def predict_data(self, dataset_x):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        print("="*80)
        print("ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        print("="*80)
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ íŠ¹ì„±ê³¼ ë™ì¼í•˜ê²Œ)
        # í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ íŠ¹ì„±: [0,1,2,7,8,13,17,19,21] = ê¸°ì´ˆê¸ˆì•¡, ë‚™ì°°í•˜í•œë¥ , ì°¸ì—¬ì—…ì²´ìˆ˜, ê°„ì ‘ë¹„, ìˆœê³µì‚¬ì›ê°€, ë©´í—ˆì œí•œì½”ë“œ, ê³µê³ ê¸°ê´€ì ìˆ˜, ê³µì‚¬ì§€ì—­ì ìˆ˜, í‚¤ì›Œë“œì ìˆ˜
        feature_columns = ['ê¸°ì´ˆê¸ˆì•¡', 'ë‚™ì°°í•˜í•œë¥ ', 'ì°¸ì—¬ì—…ì²´ìˆ˜', 'ê°„ì ‘ë¹„', 'ìˆœê³µì‚¬ì›ê°€', 
                          'ë©´í—ˆì œí•œì½”ë“œ', 'ê³µê³ ê¸°ê´€ì ìˆ˜', 'ê³µì‚¬ì§€ì—­ì ìˆ˜', 'í‚¤ì›Œë“œì ìˆ˜']
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_feature_columns = [col for col in feature_columns if col in dataset_x.columns]
        selected_columns = dataset_x[available_feature_columns]
        
        print(f"ì„ íƒëœ íŠ¹ì„± ì»¬ëŸ¼: {available_feature_columns}")
        print(f"ì„ íƒëœ ì»¬ëŸ¼ ìˆ˜: {len(available_feature_columns)}")
        
        # ë°ì´í„° ê²€ì¦
        print("ğŸ” ë°ì´í„° ê²€ì¦ ì¤‘...")
        if selected_columns.isnull().any().any():
            print("âš ï¸  ê²°ì¸¡ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            selected_columns = selected_columns.fillna(0)
        
        if selected_columns.isin([np.inf, -np.inf]).any().any():
            print("âš ï¸  ë¬´í•œê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            selected_columns = selected_columns.replace([np.inf, -np.inf], 0)
        
        # ì •ê·œí™” ì ìš©
        print("ğŸ“Š ë°ì´í„° ì •ê·œí™” ì¤‘...")
        try:
            x_scaled = self.scaler.transform(selected_columns)
            print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {x_scaled.shape}")
        except Exception as e:
            print(f"âŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            print("ìŠ¤ì¼€ì¼ëŸ¬ê°€ í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ íŠ¹ì„± ìˆ˜ì™€ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"í˜„ì¬ íŠ¹ì„± ìˆ˜: {selected_columns.shape[1]}")
            raise e
        
        # 3ê°œ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì¤‘...")
        try:
            pred1 = self.model1.predict(x_scaled)  # ì—…ì²´íˆ¬ì°°ë¥  ì˜ˆì¸¡
            print("âœ… ì—…ì²´íˆ¬ì°°ë¥  ì˜ˆì¸¡ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì—…ì²´íˆ¬ì°°ë¥  ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            pred1 = np.zeros(len(x_scaled))
        
        try:
            pred2 = self.model2.predict(x_scaled)  # ì˜ˆê°€íˆ¬ì°°ë¥  ì˜ˆì¸¡
            print("âœ… ì˜ˆê°€íˆ¬ì°°ë¥  ì˜ˆì¸¡ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì˜ˆê°€íˆ¬ì°°ë¥  ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            pred2 = np.zeros(len(x_scaled))
        
        try:
            pred3 = self.model3.predict(x_scaled)  # ì°¸ì—¬ì—…ì²´ìˆ˜ ì˜ˆì¸¡
            print("âœ… ì°¸ì—¬ì—…ì²´ìˆ˜ ì˜ˆì¸¡ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì°¸ì—¬ì—…ì²´ìˆ˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            pred3 = np.zeros(len(x_scaled))
        
        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€
        result_df = dataset_x.copy()
        
        # ì…ì°°ë²ˆí˜¸ì™€ ì…ì°°ì°¨ìˆ˜ë¥¼ ì›ë³¸ ë°ì´í„°ì—ì„œ ê°€ì ¸ì™€ì„œ ì œì¼ ì•ì— ì¶”ê°€
        original_data = pd.read_csv(self.data_dir + "sample_prediction_data.csv")
        original_data.columns = original_data.columns.str.strip()  # ì»¬ëŸ¼ëª… ê³µë°± ì œê±°
        
        if 'ì…ì°°ë²ˆí˜¸' in original_data.columns:
            result_df.insert(0, 'ì…ì°°ë²ˆí˜¸', original_data['ì…ì°°ë²ˆí˜¸'].values)
        if 'ì…ì°°ì°¨ìˆ˜' in original_data.columns:
            result_df.insert(1, 'ì…ì°°ì°¨ìˆ˜', original_data['ì…ì°°ì°¨ìˆ˜'].values)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì»¬ëŸ¼ë“¤ ì¶”ê°€
        result_df['ì—…ì²´íˆ¬ì°°ë¥ ì˜ˆì¸¡'] = pred1
        result_df['ì˜ˆê°€íˆ¬ì°°ë¥ ì˜ˆì¸¡'] = pred2
        result_df['ì°¸ì—¬ì—…ì²´ìˆ˜ì˜ˆì¸¡'] = pred3
        
        # ì¶”ê°€ ê³„ì‚° ì»¬ëŸ¼ë“¤
        result_df['ì˜ˆì •ê¸ˆì•¡ì˜ˆì¸¡'] = (result_df['ì˜ˆê°€íˆ¬ì°°ë¥ ì˜ˆì¸¡'] * result_df['ê¸°ì´ˆê¸ˆì•¡']) / result_df['ë‚™ì°°í•˜í•œë¥ ']
        result_df['ë‚™ì°°ê¸ˆì•¡(ì—…ì²´íˆ¬ì°°ë¥ )ì˜ˆì¸¡'] = result_df['ì—…ì²´íˆ¬ì°°ë¥ ì˜ˆì¸¡'] * result_df['ê¸°ì´ˆê¸ˆì•¡']
        # Aê°’ì—¬ë¶€: ê°„ì ‘ë¹„ê°€ 0ì›ì´ ì•„ë‹ ê²½ìš° O, 0ì›ì¼ ê²½ìš° X
        result_df['Aê°’ì—¬ë¶€'] = result_df['ê°„ì ‘ë¹„'].apply(lambda x: 'O' if x != 0 else 'X')
        
        print("âœ… ì˜ˆì¸¡ ì™„ë£Œ")
        print(f"   - ì—…ì²´íˆ¬ì°°ë¥  ì˜ˆì¸¡ ë²”ìœ„: {pred1.min():.3f} ~ {pred1.max():.3f}")
        print(f"   - ì˜ˆê°€íˆ¬ì°°ë¥  ì˜ˆì¸¡ ë²”ìœ„: {pred2.min():.3f} ~ {pred2.max():.3f}")
        print(f"   - ì°¸ì—¬ì—…ì²´ìˆ˜ ì˜ˆì¸¡ ë²”ìœ„: {pred3.min():.1f} ~ {pred3.max():.1f}")
        
        return result_df
    
    def save_predictions(self, result_df, output_file):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        print("="*80)
        print("ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")
        print("="*80)
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        result_dir = os.path.join(self.save_dir, "predict_result")
        os.makedirs(result_dir, exist_ok=True)
        
        output_path = os.path.join(result_dir, output_file)
        
        try:
            result_df.to_excel(
                output_path,
                sheet_name='ì˜ˆì¸¡ê²°ê³¼',
                na_rep='NaN',
                float_format="%.6f",
                header=True,
                index=True,
                index_label="id",
                freeze_panes=(1, 0)
            )
            print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(result_df)}í–‰ x {len(result_df.columns)}ì—´")
            
        except Exception as e:
            print(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")
            # CSVë¡œ ì €ì¥
            csv_path = output_path.replace('.xlsx', '.csv')
            result_df.to_csv(csv_path, 
                           na_rep='NaN', 
                           float_format="%.6f", 
                           header=True, 
                           index=True, 
                           index_label="id",
                           encoding='utf-8-sig')
            print(f"âœ… CSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {csv_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    data_file = "sample_prediction_data.csv"
    
    try:
        print("="*80)
        print("ğŸ¯ ìƒ˜í”Œ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘")
        print("="*80)
        
        # ì˜ˆì¸¡ê¸° ìƒì„±
        predictor = SampleDataPredictor()
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = predictor.preprocess_data(data_file)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = predictor.predict_data(processed_data)
        
        # ê²°ê³¼ ì €ì¥
        output_file = f"sample_prediction_result_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        predictor.save_predictions(predictions, output_file)
        
        print("="*80)
        print("ğŸ‰ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: res/predict_result/{output_file}")
        print("="*80)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(f"ì´ ì˜ˆì¸¡ ê±´ìˆ˜: {len(predictions)}")
        print(f"ì—…ì²´íˆ¬ì°°ë¥  í‰ê· : {predictions['ì—…ì²´íˆ¬ì°°ë¥ ì˜ˆì¸¡'].mean():.3f}")
        print(f"ì˜ˆê°€íˆ¬ì°°ë¥  í‰ê· : {predictions['ì˜ˆê°€íˆ¬ì°°ë¥ ì˜ˆì¸¡'].mean():.3f}")
        print(f"ì°¸ì—¬ì—…ì²´ìˆ˜ í‰ê· : {predictions['ì°¸ì—¬ì—…ì²´ìˆ˜ì˜ˆì¸¡'].mean():.1f}")
        print(f"Aê°’ ì—¬ë¶€: {predictions['Aê°’ì—¬ë¶€'].value_counts().to_dict()}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
